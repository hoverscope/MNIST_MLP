{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dcfdd883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library imports numpy, pandas and matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "067eed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset using pandas\n",
    "#df_train for training and df_test for testig purposes\n",
    "df_train  = pd.read_csv('data/dataSet1.csv', header = None)\n",
    "df_test  = pd.read_csv('data/dataSet2.csv', header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92357144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating numpy arrays train and test out of pd dataframe for easier data manipulation \n",
    "train = df_train .to_numpy()\n",
    "test = df_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f8f0834b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2810, 65)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking array shape for verfication \n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c461528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train and test data\n",
    "X_train = train[:, :64] #X_train contains all input values wich are the pixel values here\n",
    "Y_train = train[:, 64] #Y_train contains all label/output values wich are the actual numbers\n",
    "\n",
    "#similary this will be test data\n",
    "X_test = test[:, :64]\n",
    "Y_test = test[:, 64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "25425d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable to control number of neurons in the hidden layer\n",
    "h = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2cd80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate initial values for weights and biases in the network\n",
    "def init_params(h):\n",
    "    w1 = np.random.randn(h, 64) * np.sqrt(2. / (64)) #w1 weights for input layer\n",
    "    b1 = np.zeros((h, 1))   #b1 bias for input layer \n",
    "    w2 = np.random.randn(10, h) * np.sqrt(2. / (h)) #w2 weights for input layer\n",
    "    b2 = np.zeros((10, 1)) #b2 weights for input layer\n",
    "    return w1, b1, w1, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f4325387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defineing ReLU function\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0,Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "feca330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defineing Sigmoid function\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0301d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Propogation Function\n",
    "def forward_prop(w1, b1, w2, b2, X):\n",
    "    Z1 = w1.dot(X.T) + b1 #input layer dot product with bias added\n",
    "    A1 = ReLU(Z1) #ReLU activation function\n",
    "    Z2 = w2.dot(A1) + b2 #hidden layer dot product with bias added\n",
    "    A2 = sigmoid(Z2) #sigmoid activation\n",
    "   \n",
    "    return Z1, A1, Z2, A2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "47462366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping label to an array\n",
    "#ex: if label is 5 then map the 4th element as 1\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    l1 = len(Y) #get len of the array\n",
    "    l2 = 10 #10 as the numbers range from 0-9\n",
    "    one_hot_Y = np.zeros((l2, l1), dtype=int)\n",
    "    for i, label in enumerate(Y):\n",
    "        one_hot_Y[label][i] = 1\n",
    "    return one_hot_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92d97f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#back propgation function for the network to train it\n",
    "def back_prop(Z1, A1, Z2, A2, w2, X, Y):\n",
    "    m = Y.size  \n",
    "    one_hot_Y = one_hot_encode(Y)  \n",
    "    \n",
    "    dZ2 = A2 - one_hot_Y  # Error for output layer\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)  # Gradient of W2\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True) \n",
    "    \n",
    "    # Backpropagation for the hidden layer (Z1 -> A1)\n",
    "    dZ1 = np.dot(w2.T, dZ2) * (A1 > 0) \n",
    "    dW1 = (1 / m) * np.dot(dZ1, X) \n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)  \n",
    "    \n",
    "    return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9824e320",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "duplicate argument 'w1' in function definition (325169857.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[83], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    def update_params(w1, b1, w1, b2, dw1, db1, dw1, db2, alpha):\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m duplicate argument 'w1' in function definition\n"
     ]
    }
   ],
   "source": [
    "#function to update the params as we train\n",
    "#alpha is the learning rate\n",
    "def update_params(w1, b1, w2, b2, dw1, db1, dw1, db2, alpha):\n",
    "    w1 = w1 - alpha * dw1\n",
    "    b1 = b1 - alpha * db1\n",
    "    w2 = w1 - alpha * dw1\n",
    "    b2 = b2 - alpha * db2\n",
    "    return w1, b1, w2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "64f070cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get predictions\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "15cec39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get accuracy\n",
    "def get_accuracy(predictions, Y):\n",
    "    one_hot_Y = one_hot_encode(Y)\n",
    "    accuracy = np.sum(predictions == np.argmax(one_hot_Y, axis=0)) / Y.size *100\n",
    "    print(\"Predictions:\", predictions)\n",
    "    print(\"True Labels:\", Y)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b057a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, Y_train,iterations, alpha):\n",
    "    w1, b1, w2, b2 = init_params(h)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(w1, b1, w2, b2, X_train)\n",
    "        dw1, db1, dw1, db2 = back_prop(Z1, A1, Z2, A2, w1, X_train, Y_train)\n",
    "        w1, b1, w2, b2 = update_params(w1, b1, w2, b2, dw1, db1, dw1, db2, alpha)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Iteration:\", i)\n",
    "            print(\"Accuracy:\", get_accuracy(get_predictions(A2), Y_train))\n",
    "            \n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2d935ad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (128,64) and (128,2810) not aligned: 64 (dim 1) != 128 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w1, b1, w1, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[86], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X_train, Y_train, iterations, alpha)\u001b[0m\n\u001b[0;32m      2\u001b[0m w1, b1, w1, b2 \u001b[38;5;241m=\u001b[39m init_params(h)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m----> 5\u001b[0m     Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     dw1, db1, dw1, db2 \u001b[38;5;241m=\u001b[39m back_prop(Z1, A1, Z2, A2, w1, X_train, Y_train)\n\u001b[0;32m      7\u001b[0m     w1, b1, w1, b2 \u001b[38;5;241m=\u001b[39m update_params(w1, b1, w1, b2, dw1, db1, dw1, db2, alpha)\n",
      "Cell \u001b[1;32mIn[80], line 5\u001b[0m, in \u001b[0;36mforward_prop\u001b[1;34m(w1, b1, w2, b2, X)\u001b[0m\n\u001b[0;32m      3\u001b[0m Z1 \u001b[38;5;241m=\u001b[39m w1\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m b1 \u001b[38;5;66;03m#input layer dot product with bias added\u001b[39;00m\n\u001b[0;32m      4\u001b[0m A1 \u001b[38;5;241m=\u001b[39m ReLU(Z1) \u001b[38;5;66;03m#ReLU activation function\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m Z2 \u001b[38;5;241m=\u001b[39m \u001b[43mw2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b2 \u001b[38;5;66;03m#hidden layer dot product with bias added\u001b[39;00m\n\u001b[0;32m      6\u001b[0m A2 \u001b[38;5;241m=\u001b[39m sigmoid(Z2) \u001b[38;5;66;03m#sigmoid activation\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Z1, A1, Z2, A2\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (128,64) and (128,2810) not aligned: 64 (dim 1) != 128 (dim 0)"
     ]
    }
   ],
   "source": [
    "w1, b1, w2, b2 = train(X_train, Y_train,1000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, index, w1, b1, w2, b2):\n",
    "    \n",
    "    Z1, A1, Z2, A2 = forward_prop(w1, b1, w2, b2, X)\n",
    "    \n",
    "    predictions = get_predictions(A2)\n",
    "    \n",
    "    predicted_label = predictions[index]\n",
    "    \n",
    "    actual_label = Y[index]\n",
    "    \n",
    "    image_data = X[index, :64].reshape(8, 8)   \n",
    "    \n",
    "    plt.imshow(image_data, cmap='gray')\n",
    "    plt.axis('off')  # Hide the axes for a clean plot\n",
    "    plt.title(f\"Predicted: {predicted_label}, Actual: {actual_label}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the predicted and actual labels for the given index\n",
    "    print(f\"Index: {index}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "    print(f\"Actual Label: {actual_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868ac7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 91\n",
      "Predicted Label: 6\n",
      "Actual Label: 6\n"
     ]
    }
   ],
   "source": [
    "index = 91\n",
    "\n",
    "predict(X_test, Y_test, index, w1, b1, w2, b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feed181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
