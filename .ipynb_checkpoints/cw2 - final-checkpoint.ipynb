{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acefdd4",
   "metadata": {},
   "source": [
    "# MNIST MLP (Multi-Layer-Perceptron ) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f28a9",
   "metadata": {},
   "source": [
    "Multi-Layer-Perceptron Model trained on MNIST dataset. Machine learning model to categorise one of the UCI digit tasks. \n",
    "The model has one input layer, a hidden layer and an output layer. Built from scratch only using the libaray numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101529c",
   "metadata": {},
   "source": [
    "## Section 1:\n",
    "### Importing Libraries, loading the dataset and creating train and test dataset using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ecf5b",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcfdd883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library imports numpy, pandas, matplotlib and random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067eed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset using pandas\n",
    "#df_train for training and df_test for testig purposes\n",
    "df_train  = pd.read_csv('data/dataSet1.csv', header = None)\n",
    "df_test  = pd.read_csv('data/dataSet2.csv', header = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf81a5e",
   "metadata": {},
   "source": [
    "#### Checking the pandas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3179d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  55  56  57  58  59  60  61  \\\n",
       "0   0   1   6  15  12   1   0   0   0   7  ...   0   0   0   6  14   7   1   \n",
       "1   0   0  10  16   6   0   0   0   0   7  ...   0   0   0  10  16  15   3   \n",
       "2   0   0   8  15  16  13   0   0   0   1  ...   0   0   0   9  14   0   0   \n",
       "3   0   0   0   3  11  16   0   0   0   0  ...   0   0   0   0   1  15   2   \n",
       "4   0   0   5  14   4   0   0   0   0   0  ...   0   0   0   4  12  14   7   \n",
       "\n",
       "   62  63  64  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "2   0   0   7  \n",
       "3   0   0   4  \n",
       "4   0   0   6  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40743aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  55  56  57  58  59  60  61  \\\n",
       "0   0   0   7  16  16  16   2   0   0   0  ...   0   0   0   8  13  10   2   \n",
       "1   0   0  13  16   7   0   0   0   0   2  ...   0   0   0  13  16  16  12   \n",
       "2   0   0   4  15  16  16  10   0   0   0  ...   0   0   0   2  16  11   3   \n",
       "3   0   0   3  14  16  16   6   0   0   0  ...   0   0   0   7  14   2   0   \n",
       "4   0   0   0   1  15  10   0   0   0   0  ...   0   0   0   0   4  16   7   \n",
       "\n",
       "   62  63  64  \n",
       "0   0   0   0  \n",
       "1   0   0   2  \n",
       "2   0   0   5  \n",
       "3   0   0   7  \n",
       "4   0   0   4  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8b554",
   "metadata": {},
   "source": [
    "#### Creating Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92357144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating numpy arrays train and test out of pd dataframe for easier data manipulation \n",
    "train = df_train.to_numpy()\n",
    "test = df_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f0834b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2810, 65)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking array shape for verfication \n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f510c2",
   "metadata": {},
   "source": [
    "#### Creating train and test data with inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c461528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train and test data\n",
    "\n",
    "#X_train contains all input values wich are the pixel values here. \n",
    "#The first 64 coloumns of each row are the input values\n",
    "X_train = train[:, :64] \n",
    "\n",
    "#Y_train contains all label/output values wich are the actual numbers\n",
    "Y_train = train[:, 64] \n",
    "\n",
    "#similary this will be test data\n",
    "X_test = test[:, :64]\n",
    "Y_test = test[:, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b4e6c6",
   "metadata": {},
   "source": [
    "# Section 2:\n",
    "### Defining Hyperparameter Variables and functions to initialize the weights along with functions to define activation functions for input and output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61707e3",
   "metadata": {},
   "source": [
    "#### Hyperparameter Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25425d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining hyperparameter variables \n",
    "\n",
    "#h is the number of neurons in the hidden layer\n",
    "#training_cycles is number of cycles\n",
    "#learning_rate is to control the models learning rate\n",
    "\n",
    "h = 180\n",
    "training_cycles = 20000\n",
    "learning_rate = 0.009\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa065b",
   "metadata": {},
   "source": [
    "#### Initializing model weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b414e85",
   "metadata": {},
   "source": [
    "Takes in argument 'h' and initalizes the weights for the model using HE Initialzation with variance 2/n_in for ReLU activation.\n",
    "Bias values are initalized using the 'h' called b1 and b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2cd80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(h):\n",
    "    w1 = np.random.randn(h, 64) * np.sqrt(2. / (64))  # He initialization\n",
    "    b1 = np.zeros((h, 1))  #bias value in input-hidden layer\n",
    "    \n",
    "    w2 = np.random.randn(10, h) * np.sqrt(2. / (h))  # He initialization\n",
    "    b2 = np.zeros((10, 1))  #bias value for hidden-output layer\n",
    "    return w1, b1, w2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ddb15a",
   "metadata": {},
   "source": [
    "#### Defining Sigmoid and LeakyRelu activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feca330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defineing Sigmoid function\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b1f4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeakyReLU(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde0794",
   "metadata": {},
   "source": [
    "# Section 3:\n",
    "### Defining feed_forward , label encoding and backward propogation function for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d728a43",
   "metadata": {},
   "source": [
    "#### Feed Forward Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c5bdd",
   "metadata": {},
   "source": [
    "Takes in arguments w1, b1, b2 and X. \n",
    "X will X_train or training input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0301d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(w1, b1, w2, b2, X):\n",
    "    Z1 = np.dot(w1, X.T) + b1\n",
    "    A1 = LeakyReLU(Z1)\n",
    "    \n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = sigmoid(Z2)  \n",
    "    \n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37082a29",
   "metadata": {},
   "source": [
    "#### Map Labels function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bf36a",
   "metadata": {},
   "source": [
    "Maps label to an array of size 10\n",
    "Output/Label values range from 0-9. \n",
    "\n",
    "Based on label value map 1 to an array\n",
    "ex: if label is 4 then map the 4th element as 3\n",
    "\n",
    "[0,0,0,1,0,0,0,0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47462366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping label to an array\n",
    "#ex: if label is 5 then map the 4th element as 1\n",
    "\n",
    "def map_labels(Y):\n",
    "    l1 = len(Y) #get len of the array\n",
    "    l2 = 10 #10 as the numbers range from 0-9\n",
    "    mapped_label = np.zeros((l2, l1), dtype=int)\n",
    "    for i, label in enumerate(Y):\n",
    "        mapped_label[label][i] = 1\n",
    "    return mapped_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d753cc",
   "metadata": {},
   "source": [
    "#### LekyRelu derivative  function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddc74cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeakyReLU_derivative(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, 1, alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d36c47",
   "metadata": {},
   "source": [
    "#### Back Propogate  function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba6b01",
   "metadata": {},
   "source": [
    "Calculate gradients for weights and biases in the output and hidden layers using backpropagation.\n",
    "Uses Leaky ReLU derivative for the hidden layer and mapped labels for the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d97f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(Z1, A1, Z2, A2, w2, X, Y):\n",
    "    m = Y.size\n",
    "    mapped_label = map_labels(Y)\n",
    "    \n",
    "    # Backpropagation for the output layer (Z2 -> A2)\n",
    "    dZ2 = A2 - mapped_label  # Gradient for output layer\n",
    "    dw2 = (1 / m) * np.dot(dZ2, A1.T)  # Gradient for w2\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)  # Gradient for b2\n",
    "    \n",
    "    # Backpropagation for the hidden layer (Z1 -> A1)\n",
    "    dZ1 = np.dot(w2.T, dZ2) * LeakyReLU_derivative(Z1)  # Leaky ReLU derivative\n",
    "    dw1 = (1 / m) * np.dot(dZ1, X)  # Gradient for w1\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)  # Gradient for b1\n",
    "    \n",
    "    return dw1, db1, dw2, db2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0eace2",
   "metadata": {},
   "source": [
    "#### Update Params Fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9824e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to update the params as we train\n",
    "#alpha is the learning rate\n",
    "\n",
    "def update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha):\n",
    "    w1 = w1 - alpha * dw1\n",
    "    b1 = b1 - alpha * db1\n",
    "    w2 = w2 - alpha * dw2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return w1, b1, w2, b2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8032e0",
   "metadata": {},
   "source": [
    "# Section 4:\n",
    "### Defining prediction and accuracy functions\n",
    "### Training the MLP Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d0ba7",
   "metadata": {},
   "source": [
    "#### Prediction and Accuracy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64f070cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    print(\"Predictions (using np.argmax):\", np.argmax(A2, axis=0))  # Debugging\n",
    "    return np.argmax(A2, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15cec39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions, Y):\n",
    "    mapped_label = map_labels(Y)\n",
    "    accuracy = np.sum(predictions == np.argmax(mapped_label, axis=0)) / Y.size *100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2b301",
   "metadata": {},
   "source": [
    "#### Training the Network with RMS Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a24e6",
   "metadata": {},
   "source": [
    "Training function is definied below with RMS Prop optimizer\n",
    "\n",
    "Train the neural network using RMSprop optimization and dynamic learning rate adjustment. \n",
    "Includes weight decay for L2 regularization and early stopping based on accuracy improvement.\n",
    "Periodically evaluates test accuracy and saves the best model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743f538",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_network(X_train, Y_train, X_test, Y_test, iterations, learning_rate):\n",
    "    # Initialize parameters\n",
    "    w1, b1, w2, b2 = init_params(h)  # h is the hidden layer size\n",
    "    \n",
    "    # RMSprop parameters\n",
    "    epsilon = 1e-8\n",
    "    weight_decay = 1e-4  # L2 regularization\n",
    "    \n",
    "    # RMSprop specific variables\n",
    "    cache_w1, cache_b1, cache_w2, cache_b2 = np.zeros_like(w1), np.zeros_like(b1), np.zeros_like(w2), np.zeros_like(b2)\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    patience = 10\n",
    "    no_improve_counter = 0\n",
    "    \n",
    "    best_w1, best_b1, best_w2, best_b2 = w1.copy(), b1.copy(), w2.copy(), b2.copy()  \n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        # Dynamic learning rate adjustment\n",
    "        lr = learning_rate * (0.99 ** (iteration // 500))\n",
    "        \n",
    "        # Forward and backward pass\n",
    "        Z1, A1, Z2, A2 = feed_forward(w1, b1, w2, b2, X_train)\n",
    "        dw1, db1, dw2, db2 = back_prop(Z1, A1, Z2, A2, w2, X_train, Y_train)\n",
    "        \n",
    "        # Add weight decay (L2 regularization)\n",
    "        dw1 += weight_decay * w1\n",
    "        dw2 += weight_decay * w2\n",
    "        \n",
    "        # RMSprop optimization with squared gradients\n",
    "        cache_w1 = 0.9 * cache_w1 + 0.1 * (dw1 ** 2)\n",
    "        cache_b1 = 0.9 * cache_b1 + 0.1 * (db1 ** 2)\n",
    "        cache_w2 = 0.9 * cache_w2 + 0.1 * (dw2 ** 2)\n",
    "        cache_b2 = 0.9 * cache_b2 + 0.1 * (db2 ** 2)\n",
    "        \n",
    "        # Parameter updates with RMSprop\n",
    "        w1 -= lr * dw1 / (np.sqrt(cache_w1) + epsilon)\n",
    "        b1 -= lr * db1 / (np.sqrt(cache_b1) + epsilon)\n",
    "        w2 -= lr * dw2 / (np.sqrt(cache_w2) + epsilon)\n",
    "        b2 -= lr * db2 / (np.sqrt(cache_b2) + epsilon)\n",
    "        \n",
    "        # Periodic evaluation every 100 iterations\n",
    "        if iteration % 100 == 0:\n",
    "            # Evaluate on training data\n",
    "            Z1_train, A1_train, Z2_train, A2_train = feed_forward(w1, b1, w2, b2, X_train)\n",
    "            train_predictions = get_predictions(A2_train)\n",
    "            train_accuracy = get_accuracy(train_predictions, Y_train)\n",
    "            \n",
    "            Z1_test, A1_test, Z2_test, A2_test = feed_forward(w1, b1, w2, b2, X_test)\n",
    "            test_predictions = get_predictions(A2_test)\n",
    "            test_accuracy = get_accuracy(test_predictions, Y_test)\n",
    "            \n",
    "            print(f\"Iteration {iteration}: Train Accuracy {train_accuracy:.2f}% | Test Accuracy {test_accuracy:.2f}% | Learning rate: {lr:.6f} | Weight Decay: {weight_decay:.6f}\")\n",
    "            \n",
    "            # Update best accuracy and monitor no improvement\n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                best_w1, best_b1, best_w2, best_b2 = w1.copy(), b1.copy(), w2.copy(), b2.copy()  # Save the best model\n",
    "                no_improve_counter = 0\n",
    "            else:\n",
    "                no_improve_counter += 1\n",
    "            \n",
    "            # If we reach taget accuracy, stop\n",
    "            if test_accuracy >= 98.4:\n",
    "                print(f\"Achieved target accuracy of 98% at iteration {iteration}.\")\n",
    "                break\n",
    "            \n",
    "            # If no improvement after 'patience' iterations, adjust hyperparameters\n",
    "            if no_improve_counter > patience:\n",
    "                print(f\"No improvement for {patience} iterations. Adjusting hyperparameters...\")\n",
    "                weight_decay *= 0.5  # weight decay \n",
    "                learning_rate *= 0.9  # learning rate updates\n",
    "                no_improve_counter = 0  # Reset the counter\n",
    "    \n",
    "    # Once training is over, return the best model\n",
    "    return best_w1, best_b1, best_w2, best_b2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807cc814",
   "metadata": {},
   "source": [
    "#### Training the model and calling the fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7c1c96f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (using np.argmax): [2 2 2 ... 2 2 2]\n",
      "Predictions (using np.argmax): [2 2 2 ... 2 2 2]\n",
      "Iteration 0: Train Accuracy 13.49% | Test Accuracy 13.17% | Learning rate: 0.009000 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [8 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 3 3 ... 8 3 8]\n",
      "Iteration 100: Train Accuracy 73.06% | Test Accuracy 71.78% | Learning rate: 0.009000 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 200: Train Accuracy 96.37% | Test Accuracy 93.67% | Learning rate: 0.009000 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 300: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.009000 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 400: Train Accuracy 100.00% | Test Accuracy 97.26% | Learning rate: 0.009000 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 500: Train Accuracy 100.00% | Test Accuracy 97.33% | Learning rate: 0.008910 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 600: Train Accuracy 100.00% | Test Accuracy 97.51% | Learning rate: 0.008910 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 700: Train Accuracy 100.00% | Test Accuracy 97.44% | Learning rate: 0.008910 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 800: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.008910 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 900: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.008910 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 1000: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.008821 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 1100: Train Accuracy 100.00% | Test Accuracy 97.62% | Learning rate: 0.008821 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 1200: Train Accuracy 99.93% | Test Accuracy 97.01% | Learning rate: 0.008821 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 1300: Train Accuracy 100.00% | Test Accuracy 97.62% | Learning rate: 0.008821 | Weight Decay: 0.000100\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 1400: Train Accuracy 100.00% | Test Accuracy 97.69% | Learning rate: 0.008821 | Weight Decay: 0.000100\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 1500: Train Accuracy 100.00% | Test Accuracy 97.69% | Learning rate: 0.007859 | Weight Decay: 0.000050\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 1600: Train Accuracy 100.00% | Test Accuracy 97.51% | Learning rate: 0.007859 | Weight Decay: 0.000050\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 1700: Train Accuracy 99.96% | Test Accuracy 97.58% | Learning rate: 0.007859 | Weight Decay: 0.000050\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 1800: Train Accuracy 100.00% | Test Accuracy 97.69% | Learning rate: 0.007859 | Weight Decay: 0.000050\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 1900: Train Accuracy 100.00% | Test Accuracy 97.54% | Learning rate: 0.007859 | Weight Decay: 0.000050\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 2000: Train Accuracy 100.00% | Test Accuracy 97.65% | Learning rate: 0.007781 | Weight Decay: 0.000050\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 2100: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.007781 | Weight Decay: 0.000050\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 2200: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.007781 | Weight Decay: 0.000050\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 2300: Train Accuracy 100.00% | Test Accuracy 97.69% | Learning rate: 0.007781 | Weight Decay: 0.000050\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 2400: Train Accuracy 100.00% | Test Accuracy 97.69% | Learning rate: 0.007781 | Weight Decay: 0.000050\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 2500: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.007703 | Weight Decay: 0.000050\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 2600: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.006933 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 2700: Train Accuracy 100.00% | Test Accuracy 98.08% | Learning rate: 0.006933 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 2800: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.006933 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 2900: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.006933 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 3000: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.006863 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 3100: Train Accuracy 100.00% | Test Accuracy 97.65% | Learning rate: 0.006863 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 3200: Train Accuracy 100.00% | Test Accuracy 97.69% | Learning rate: 0.006863 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 3300: Train Accuracy 100.00% | Test Accuracy 97.65% | Learning rate: 0.006863 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 3400: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.006863 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 3500: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.006795 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 3600: Train Accuracy 100.00% | Test Accuracy 98.01% | Learning rate: 0.006795 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 3700: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.006795 | Weight Decay: 0.000025\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 3800: Train Accuracy 100.00% | Test Accuracy 98.04% | Learning rate: 0.006795 | Weight Decay: 0.000025\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 3900: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.006115 | Weight Decay: 0.000013\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 4000: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.006054 | Weight Decay: 0.000013\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 4100: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.006054 | Weight Decay: 0.000013\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 4200: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.006054 | Weight Decay: 0.000013\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 4300: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.006054 | Weight Decay: 0.000013\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 4400: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.006054 | Weight Decay: 0.000013\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 4500: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.005994 | Weight Decay: 0.000013\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 4600: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.005994 | Weight Decay: 0.000013\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 4700: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.005994 | Weight Decay: 0.000013\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 4800: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.005994 | Weight Decay: 0.000013\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 4900: Train Accuracy 100.00% | Test Accuracy 97.65% | Learning rate: 0.005994 | Weight Decay: 0.000013\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 5000: Train Accuracy 99.86% | Test Accuracy 96.87% | Learning rate: 0.005340 | Weight Decay: 0.000006\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 5100: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.005340 | Weight Decay: 0.000006\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 5200: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.005340 | Weight Decay: 0.000006\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 5300: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.005340 | Weight Decay: 0.000006\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 5400: Train Accuracy 100.00% | Test Accuracy 98.08% | Learning rate: 0.005340 | Weight Decay: 0.000006\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 5500: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.005287 | Weight Decay: 0.000006\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 5600: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.005287 | Weight Decay: 0.000006\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 5700: Train Accuracy 100.00% | Test Accuracy 98.01% | Learning rate: 0.005287 | Weight Decay: 0.000006\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 5800: Train Accuracy 100.00% | Test Accuracy 98.01% | Learning rate: 0.005287 | Weight Decay: 0.000006\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 5900: Train Accuracy 100.00% | Test Accuracy 98.08% | Learning rate: 0.005287 | Weight Decay: 0.000006\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 6000: Train Accuracy 100.00% | Test Accuracy 98.08% | Learning rate: 0.005234 | Weight Decay: 0.000006\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 6100: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.004711 | Weight Decay: 0.000003\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 6200: Train Accuracy 100.00% | Test Accuracy 98.04% | Learning rate: 0.004711 | Weight Decay: 0.000003\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 6300: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.004711 | Weight Decay: 0.000003\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 6400: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.004711 | Weight Decay: 0.000003\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 6500: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.004664 | Weight Decay: 0.000003\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 6600: Train Accuracy 100.00% | Test Accuracy 98.08% | Learning rate: 0.004664 | Weight Decay: 0.000003\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 6700: Train Accuracy 100.00% | Test Accuracy 98.08% | Learning rate: 0.004664 | Weight Decay: 0.000003\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 6800: Train Accuracy 100.00% | Test Accuracy 98.08% | Learning rate: 0.004664 | Weight Decay: 0.000003\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 6900: Train Accuracy 100.00% | Test Accuracy 98.04% | Learning rate: 0.004664 | Weight Decay: 0.000003\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 7000: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.004617 | Weight Decay: 0.000003\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 7100: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.004617 | Weight Decay: 0.000003\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 7200: Train Accuracy 100.00% | Test Accuracy 98.15% | Learning rate: 0.004155 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 7300: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.004155 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 7400: Train Accuracy 100.00% | Test Accuracy 98.04% | Learning rate: 0.004155 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 7500: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.004114 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 7600: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.004114 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 7700: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.004114 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 7800: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.004114 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 7900: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.004114 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 8000: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.004072 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 8100: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.004072 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 8200: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.004072 | Weight Decay: 0.000002\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 8300: Train Accuracy 100.00% | Test Accuracy 98.01% | Learning rate: 0.004072 | Weight Decay: 0.000002\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 8400: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.003665 | Weight Decay: 0.000001\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 8500: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.003629 | Weight Decay: 0.000001\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 8600: Train Accuracy 100.00% | Test Accuracy 97.69% | Learning rate: 0.003629 | Weight Decay: 0.000001\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 8700: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.003629 | Weight Decay: 0.000001\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 8800: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.003629 | Weight Decay: 0.000001\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 8900: Train Accuracy 100.00% | Test Accuracy 98.01% | Learning rate: 0.003629 | Weight Decay: 0.000001\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 9000: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.003592 | Weight Decay: 0.000001\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 9100: Train Accuracy 100.00% | Test Accuracy 97.44% | Learning rate: 0.003592 | Weight Decay: 0.000001\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 9200: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.003592 | Weight Decay: 0.000001\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 9300: Train Accuracy 100.00% | Test Accuracy 98.04% | Learning rate: 0.003592 | Weight Decay: 0.000001\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 9400: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.003592 | Weight Decay: 0.000001\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 9500: Train Accuracy 100.00% | Test Accuracy 97.65% | Learning rate: 0.003201 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 9600: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.003201 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 9700: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.003201 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 9800: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.003201 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 9900: Train Accuracy 100.00% | Test Accuracy 98.01% | Learning rate: 0.003201 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 10000: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.003169 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 10100: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.003169 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 10200: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.003169 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 10300: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.003169 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 10400: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.003169 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 10500: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.003137 | Weight Decay: 0.000000\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 10600: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.002823 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 10700: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.002823 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 10800: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.002823 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 10900: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.002823 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 11000: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.002795 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 11100: Train Accuracy 100.00% | Test Accuracy 97.69% | Learning rate: 0.002795 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 11200: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.002795 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 11300: Train Accuracy 100.00% | Test Accuracy 97.65% | Learning rate: 0.002795 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 11400: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.002795 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 11500: Train Accuracy 100.00% | Test Accuracy 97.69% | Learning rate: 0.002767 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 11600: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.002767 | Weight Decay: 0.000000\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 11700: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.002490 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 11800: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.002490 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 11900: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.002490 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 12000: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.002466 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 12100: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.002466 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 12200: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.002466 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 12300: Train Accuracy 100.00% | Test Accuracy 97.69% | Learning rate: 0.002466 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 12400: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.002466 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 12500: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.002441 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 12600: Train Accuracy 100.00% | Test Accuracy 97.65% | Learning rate: 0.002441 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 12700: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.002441 | Weight Decay: 0.000000\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 12800: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.002197 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 12900: Train Accuracy 100.00% | Test Accuracy 97.65% | Learning rate: 0.002197 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 13000: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.002175 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 13100: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.002175 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 13200: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.002175 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 13300: Train Accuracy 100.00% | Test Accuracy 97.65% | Learning rate: 0.002175 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 13400: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.002175 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 13500: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.002153 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 13600: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.002153 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 13700: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.002153 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 13800: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.002153 | Weight Decay: 0.000000\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 13900: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.001938 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 14000: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.001918 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 14100: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.001918 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 14200: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001918 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 14300: Train Accuracy 100.00% | Test Accuracy 97.94% | Learning rate: 0.001918 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 14400: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.001918 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 14500: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.001899 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 14600: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.001899 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 14700: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001899 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 14800: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.001899 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 14900: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001899 | Weight Decay: 0.000000\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 15000: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001692 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 15100: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.001692 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 15200: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001692 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 15300: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.001692 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 15400: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001692 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 15500: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001675 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 15600: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001675 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 15700: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001675 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 15800: Train Accuracy 100.00% | Test Accuracy 97.58% | Learning rate: 0.001675 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 15900: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001675 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 16000: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.001659 | Weight Decay: 0.000000\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 16100: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001493 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 16200: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001493 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 16300: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001493 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 16400: Train Accuracy 100.00% | Test Accuracy 97.62% | Learning rate: 0.001493 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 16500: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001478 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 16600: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.001478 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 16700: Train Accuracy 100.00% | Test Accuracy 97.62% | Learning rate: 0.001478 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 16800: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001478 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 16900: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.001478 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 17000: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.001463 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 17100: Train Accuracy 100.00% | Test Accuracy 97.86% | Learning rate: 0.001463 | Weight Decay: 0.000000\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 17200: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001317 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 17300: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.001317 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 17400: Train Accuracy 100.00% | Test Accuracy 97.97% | Learning rate: 0.001317 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 17500: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001304 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 17600: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001304 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 17700: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.001304 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 17800: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001304 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 17900: Train Accuracy 100.00% | Test Accuracy 97.65% | Learning rate: 0.001304 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 18000: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001290 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 18100: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.001290 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 18200: Train Accuracy 100.00% | Test Accuracy 97.90% | Learning rate: 0.001290 | Weight Decay: 0.000000\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 18300: Train Accuracy 100.00% | Test Accuracy 97.83% | Learning rate: 0.001161 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 18400: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.001161 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 18500: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001150 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 18600: Train Accuracy 100.00% | Test Accuracy 97.54% | Learning rate: 0.001150 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 18700: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001150 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 18800: Train Accuracy 100.00% | Test Accuracy 97.76% | Learning rate: 0.001150 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 18900: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001150 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 19000: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001138 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 19100: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001138 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 19200: Train Accuracy 100.00% | Test Accuracy 97.26% | Learning rate: 0.001138 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 19300: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001138 | Weight Decay: 0.000000\n",
      "No improvement for 10 iterations. Adjusting hyperparameters...\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 19400: Train Accuracy 100.00% | Test Accuracy 97.72% | Learning rate: 0.001024 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 19500: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001014 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 19600: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001014 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 19700: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001014 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 19800: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001014 | Weight Decay: 0.000000\n",
      "Predictions (using np.argmax): [0 0 7 ... 8 1 7]\n",
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Iteration 19900: Train Accuracy 100.00% | Test Accuracy 97.79% | Learning rate: 0.001014 | Weight Decay: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Execute with RMSprop optimization and train the model\n",
    "best_w1, best_b1, best_w2, best_b2 = train_network(X_train, Y_train, X_test, Y_test, iterations=training_cycles, learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028dd0b",
   "metadata": {},
   "source": [
    "# Section 5: Evaluation\n",
    "### Finally evaluating the Model and getting is accuracy on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95f866",
   "metadata": {},
   "source": [
    "#### Function to Evaluate the model and calling accuracy fucntion to get performance results on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec946dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(X, Y, w1, b1, w2, b2):\n",
    "    Z1, A1, Z2, A2 = feed_forward(w1, b1, w2, b2, X) \n",
    "    predictions = get_predictions(A2)                \n",
    "    accuracy = get_accuracy(predictions, Y)          \n",
    "    print(f\"Accuracy on Test Data: {accuracy:.2f}%\")  \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d52805",
   "metadata": {},
   "source": [
    "#### Final Accuracy score on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0beb7b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n",
      "Accuracy on Test Data: 98.15%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model_eval(X_test, Y_test,best_w1, best_b1, best_w2, best_b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8df1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_predictions(w1, b1, w2, b2, X_test):\n",
    "    Z1, A1, Z2, A2 = feed_forward(w1, b1, w2, b2, X_test)\n",
    "    predictions = get_predictions(A2)  \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d483f2",
   "metadata": {},
   "source": [
    "#### Fucntion to select 4 random samples from test data, get actual vs predicted values and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd08820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(X_test, Y_test, w1, b1, w2, b2, num_samples=4):\n",
    "    random_indices = random.sample(range(X_test.shape[0]), num_samples)\n",
    "    selected_X = X_test[random_indices, :]\n",
    "    selected_Y = Y_test[random_indices]\n",
    "\n",
    "    Z1, A1, Z2, A2 = feed_forward(w1, b1, w2, b2, selected_X)\n",
    "    predictions = get_predictions(A2)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(selected_X[i].reshape(8, 8), cmap='gray')\n",
    "        plt.title(f\"Actual: {selected_Y[i]}\\nPredicted: {predictions[i]}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bba0e238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (using np.argmax): [3 1 2 7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(X_test, Y_test, best_w1, best_b1, best_w2, best_b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28036120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(X_test, Y_test, w1, b1, w2, b2):\n",
    "    \n",
    "    predictions = perform_predictions(best_w1, best_b1, best_w2, best_b2,X_test)\n",
    "\n",
    "    # Number of classes \n",
    "    num_classes = len(np.unique(Y_test))\n",
    "    \n",
    "    # Create the confusion matrix\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for true_label, pred_label in zip(Y_test, predictions):\n",
    "        confusion_matrix[true_label, pred_label] += 1\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a1c772c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (using np.argmax): [0 2 5 ... 8 9 8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(X_test, Y_test, best_w1, best_b1, best_w2, best_b2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
